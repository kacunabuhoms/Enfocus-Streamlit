{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efb37f5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e144f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports  \n",
    "import requests  \n",
    "import mimetypes  \n",
    "import math  \n",
    "from typing import List, Optional  \n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "import joblib  \n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor  \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV  \n",
    "from sklearn.metrics import (precision_recall_curve, mean_absolute_error, mean_squared_error,  \n",
    "                             r2_score, classification_report, confusion_matrix, accuracy_score,  \n",
    "                             roc_curve, auc, f1_score)  \n",
    "from sklearn.base import BaseEstimator, TransformerMixin  \n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler  \n",
    "from sklearn.exceptions import NotFittedError  \n",
    "from sklearn.utils.validation import check_is_fitted  \n",
    "\n",
    "from scipy.spatial import cKDTree  \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "449a5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notificar_telegram(\n",
    "    extra_msg: Optional[str] = None,\n",
    "    media_files: Optional[List[str]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Envía un mensaje por Telegram y opcionalmente adjunta archivos.\n",
    "\n",
    "    Parámetros:\n",
    "    - extra_msg: Texto adicional. Si se proporciona, se coloca\n",
    "      dos renglones después del mensaje base.\n",
    "    - media_files: Lista de rutas de fichero a enviar\n",
    "      (imágenes, CSV, PDF, etc.).\n",
    "    \"\"\"\n",
    "    BOT_TOKEN = \"7812752828:AAFiX6Fi8I-LCspDwwced4Vg9W3VgAxLwMM\"\n",
    "    CHAT_ID   = \"7914812338\"\n",
    "    BASE_MSG  = \"✅ Código completado correctamente.\"\n",
    "\n",
    "    # Construir el texto final\n",
    "    if extra_msg:\n",
    "        text = f\"{BASE_MSG}\\n\\n{extra_msg}\"\n",
    "    else:\n",
    "        text = BASE_MSG\n",
    "\n",
    "    # 1) Enviar el texto\n",
    "    url_text = f\"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\"\n",
    "    resp = requests.post(url_text, data={\"chat_id\": CHAT_ID, \"text\": text})\n",
    "    if resp.status_code != 200:\n",
    "        print(\"❌ Error al enviar mensaje de texto:\", resp.text)\n",
    "    else:\n",
    "        print(\"✅ Mensaje de texto enviado.\")\n",
    "\n",
    "    # 2) Enviar archivos uno a uno si los hay\n",
    "    if media_files:\n",
    "        for filepath in media_files:\n",
    "            mime_type, _ = mimetypes.guess_type(filepath)\n",
    "            # Elegir endpoint según tipo\n",
    "            if mime_type and mime_type.startswith(\"image/\"):\n",
    "                url_media = f\"https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto\"\n",
    "                key = \"photo\"\n",
    "            else:\n",
    "                # Para tablas (CSV), documentos (PDF, XLSX...) o cualquier otro\n",
    "                url_media = f\"https://api.telegram.org/bot{BOT_TOKEN}/sendDocument\"\n",
    "                key = \"document\"\n",
    "\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                files = { key: f }\n",
    "                data  = { \"chat_id\": CHAT_ID }\n",
    "                resp = requests.post(url_media, data=data, files=files)\n",
    "                if resp.status_code != 200:\n",
    "                    print(f\"❌ Error al enviar {filepath}:\", resp.text)\n",
    "                else:\n",
    "                    print(f\"✅ Archivo enviado: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81fbe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Bloque 2: Funciones de normalización y conversión  \n",
    "def convert_service(service: str) -> str:\n",
    "    aereo_service = [\n",
    "        \"1 kg\",\"2 kg\",\"3 kg\",\"4 kg\",\"5 kg\",\"6 kg\",\"7 kg\",\"8 kg\",\"9 kg\",\n",
    "        \"10 kg\",\"11 kg\",\"12 kg\",\"13 kg\",\"14 kg\",\"15 kg\",\"16 kg\",\"17 kg\",\n",
    "        \"19 kg\",\"20 kg\",\"21 kg\",\"22 kg\",\"24 kg\",\"25 kg\",\"26 kg\",\"27 kg\",\n",
    "        \"FEDEX_EXPRESS_SAVER\",\"FEDEX_EXPRESS_SAVER_Z1\",\"FEDEX_EXPRESS_SAVER_Z2\",\n",
    "        \"FEDEX_EXPRESS_SAVER_Z3\",\"FEDEX_EXPRESS_SAVER_Z4\",\"FEDEX_EXPRESS_SAVER_Z5\",\n",
    "        \"FEDEX_EXPRESS_SAVER_Z6\",\"FEDEX_EXPRESS_SAVER_Z7\",\"FEDEX_EXPRESS_SAVER_Z8\",\n",
    "        \"UPS_STANDAR\",\"UPS_SAVER\",\"Standard\",\"standard\",\n",
    "        \"STANDARD_ECOMMERCE_Z1\",\"STANDARD_ECOMMERCE_Z2\",\"STANDARD_ECOMMERCE_Z3\",\n",
    "        \"STANDARD_ECOMMERCE_Z4\",\"STANDARD_ECOMMERCE_Z5\",\"STANDARD_ECOMMERCE_Z6\",\n",
    "        \"STANDARD_ECOMMERCE_Z7\",\"STANDARD_ECOMMERCE_Z8\",\"STANDARD_OVERNIGHT\",\n",
    "        \"STANDARD_OVERNIGHT_Z4\",\"STANDARD_OVERNIGHT_Z6\",\n",
    "        \"STANDARD_SPECIAL_Z1\",\"STANDARD_SPECIAL_Z2\",\"STANDARD_SPECIAL_Z3\",\n",
    "        \"STANDARD_SPECIAL_Z4\",\"STANDARD_SPECIAL_Z5\",\"STANDARD_SPECIAL_Z6\",\n",
    "        \"STANDARD_SPECIAL_Z7\",\"STANDARD_Z1\",\"STANDARD_Z2\",\"STANDARD_Z3\",\n",
    "        \"STANDARD_Z4\",\"STANDARD_Z5\",\"EXPRESS DOMESTIC\",\"ECONOMY SELECT DOMESTIC\",\n",
    "        \"EXPRESS_SPECIAL_Z1\",\"EXPRESS_SPECIAL_Z2\",\"EXPRESS_SPECIAL_Z3\",\n",
    "        \"EXPRESS_SPECIAL_Z4\",\"EXPRESS_SPECIAL_Z5\",\"EXPRESS_SPECIAL_Z6\",\n",
    "        \"EXPRESS_SPECIAL_Z7\",\"EXPRESS_ECOMMERCE_Z1\",\"EXPRESS_ECOMMERCE_Z2\",\n",
    "        \"EXPRESS_ECOMMERCE_Z3\",\"EXPRESS_ECOMMERCE_Z4\",\"EXPRESS_ECOMMERCE_Z5\",\n",
    "        \"EXPRESS_ECOMMERCE_Z6\",\"EXPRESS_ECOMMERCE_Z7\",\"EXPRESS_ECOMMERCE_Z8\",\n",
    "        \"Terrestre\",\"Dia Sig.\",\"nextday\",\"economico\",\"Metropoli\",\n",
    "        \"ground\",\"saver\",\"pickup\",\"SENDEX\"\n",
    "    ]\n",
    "    return \"aereo\" if service in aereo_service else \"terrestre\"\n",
    "\n",
    "def convert_carrier(carrier: str) -> str:\n",
    "    carrier_convert = {\n",
    "        \"Afimex\": \"Afimex\",\n",
    "        \"buho\": \"Buho\",\n",
    "        \"DHL\": \"DHL\",\n",
    "        \"Estafeta\": \"Estafeta\",\n",
    "        \"FDXM\": \"Fedex\",\n",
    "        \"FEDEX MEXICO\": \"Fedex\",\n",
    "        \"FEDEX\": \"Fedex\",\n",
    "        \"fedex\": \"Fedex\",\n",
    "        \"JT Express\": \"JT Express\",\n",
    "        \"JTEX\": \"JT Express\",\n",
    "        \"Paquetexpress\": \"Paquetexpress\",\n",
    "        \"PAQUETEXPRESS\": \"Paquetexpress\",\n",
    "        \"SENDEX\": \"Sendex\",\n",
    "        \"UPS\": \"UPS\"\n",
    "    }\n",
    "    return carrier_convert.get(carrier, carrier)\n",
    "\n",
    "def convert_estado(cp: int) -> str:\n",
    "    estados = [\n",
    "        {'name':'Ciudad de México',   'min':1000,  'max':16900},\n",
    "        {'name':'Aguascalientes',     'min':20000, 'max':20997},\n",
    "        {'name':'Baja California',    'min':21000, 'max':22997},\n",
    "        {'name':'Baja California Sur','min':23000, 'max':23997},\n",
    "        {'name':'Campeche',           'min':24000, 'max':24940},\n",
    "        {'name':'Coahuila',           'min':25000, 'max':27999},\n",
    "        {'name':'Colima',             'min':28000, 'max':28989},\n",
    "        {'name':'Chiapas',            'min':29000, 'max':30997},\n",
    "        {'name':'Chihuahua',          'min':31000, 'max':33997},\n",
    "        {'name':'Durango',            'min':34000, 'max':35987},\n",
    "        {'name':'Guanajuato',         'min':36000, 'max':38997},\n",
    "        {'name':'Guerrero',           'min':39000, 'max':41998},\n",
    "        {'name':'Hidalgo',            'min':42000, 'max':43998},\n",
    "        {'name':'Jalisco',            'min':44100, 'max':49996},\n",
    "        {'name':'México',             'min':50000, 'max':57950},\n",
    "        {'name':'Michoacán',          'min':58000, 'max':61998},\n",
    "        {'name':'Morelos',            'min':62000, 'max':62996},\n",
    "        {'name':'Nayarit',            'min':63000, 'max':63996},\n",
    "        {'name':'Nuevo León',         'min':64000, 'max':67996},\n",
    "        {'name':'Oaxaca',             'min':68000, 'max':71998},\n",
    "        {'name':'Puebla',             'min':72000, 'max':75997},\n",
    "        {'name':'Querétaro',          'min':76000, 'max':76998},\n",
    "        {'name':'Quintana Roo',       'min':77000, 'max':77997},\n",
    "        {'name':'San Luis Potosí',    'min':78000, 'max':79998},\n",
    "        {'name':'Sinaloa',            'min':80000, 'max':82996},\n",
    "        {'name':'Sonora',             'min':83000, 'max':85994},\n",
    "        {'name':'Tabasco',            'min':86000, 'max':86998},\n",
    "        {'name':'Tamaulipas',         'min':87000, 'max':89970},\n",
    "        {'name':'Tlaxcala',           'min':90000, 'max':90990},\n",
    "        {'name':'Veracruz',           'min':91000, 'max':96998},\n",
    "        {'name':'Yucatán',            'min':97000, 'max':97990},\n",
    "        {'name':'Zacatecas',          'min':98000, 'max':99998}\n",
    "    ]\n",
    "    for e in estados:\n",
    "        if e['min'] <= cp <= e['max']:\n",
    "            return e['name']\n",
    "    closest = min(estados, key=lambda e: abs(cp - ((e['min'] + e['max'])/2)))\n",
    "    return closest['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3387d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 4: Carga y limpieza inicial\n",
    "def get_db():\n",
    "    url = \"https://wapi.wing.buhologistics.com/getDatasetInfo\"\n",
    "    headers = {'api-key': '0DJB9c_xpbQbprsg7iZLaUR'}\n",
    "    return requests.get(url, headers=headers).json()[\"data\"]\n",
    "\n",
    "df = pd.DataFrame(get_db()).dropna()\n",
    "df.drop(columns=[\"client\",\"coordinates_origin_pc\",\"coordinates_dest_pc\"], errors=\"ignore\", inplace=True)\n",
    "df['service_mode'] = df['service'].apply(convert_service)\n",
    "df['carrier']      = df['carrier'].apply(convert_carrier)\n",
    "df['origin_state'] = df['origin_pc'].astype(int).apply(convert_estado)\n",
    "df['dest_state']   = df['dest_pc'].astype(int).apply(convert_estado)\n",
    "\n",
    "# días hábiles\n",
    "start = pd.to_datetime(df['start_date']).values.astype('datetime64[D]')\n",
    "end   = pd.to_datetime(df['delivery_date']).values.astype('datetime64[D]')\n",
    "df['delivery_time_bd'] = np.busday_count(start, end)\n",
    "\n",
    "# filtros\n",
    "df = df[(df['delivery_time_bd']>=0) & (df['delivery_time_bd']<=10) & (df['rate']<=20)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a2838b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 5: Geolocalización\n",
    "df_coord     = pd.read_excel(\"coordenadas_mx.xlsx\")\n",
    "postal_codes = df_coord['codigo_postal'].values.reshape(-1,1)\n",
    "coords       = df_coord[['latitud','longitud']].values\n",
    "tree         = cKDTree(postal_codes)\n",
    "\n",
    "def lookup_coord(cp:int):\n",
    "    m = df_coord[df_coord['codigo_postal']==cp]\n",
    "    if not m.empty:\n",
    "        return m.iloc[0][['latitud','longitud']].tolist()\n",
    "    _, idx = tree.query([[cp]])\n",
    "    return coords[idx[0]].tolist()\n",
    "\n",
    "def haversine_vec(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    φ1,φ2 = np.radians(lat1), np.radians(lat2)\n",
    "    Δφ    = np.radians(lat2-lat1)\n",
    "    Δλ    = np.radians(lon2-lon1)\n",
    "    a     = np.sin(Δφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(Δλ/2)**2\n",
    "    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c96d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 6: Transformer & Preprocessor\n",
    "class RawToFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for _, r in X.iterrows():\n",
    "            cp_o, cp_d = int(r[\"origin_pc\"]), int(r[\"dest_pc\"])\n",
    "            fecha      = pd.to_datetime(r[\"start_date\"])\n",
    "            lo, ld     = lookup_coord(cp_o), lookup_coord(cp_d)\n",
    "            dist       = round(haversine_vec(lo[0], lo[1], ld[0], ld[1]), 2)\n",
    "            d = {\n",
    "                \"rate\":          float(r[\"rate\"]),\n",
    "                \"distance\":      dist,\n",
    "                \"origin_state\":  r[\"origin_state\"],\n",
    "                \"dest_state\":    r[\"dest_state\"],\n",
    "                \"carrier_service\": f\"{r['carrier']}_{r['service_mode']}\",\n",
    "                \"day_week\":      fecha.weekday(),\n",
    "                \"month\":         fecha.month\n",
    "            }\n",
    "            d[\"day_sin\"]    = np.sin(2*np.pi * d[\"day_week\"]/7)\n",
    "            d[\"day_cos\"]    = np.cos(2*np.pi * d[\"day_week\"]/7)\n",
    "            d[\"month_sin\"]  = np.sin(2*np.pi * d[\"month\"]/12)\n",
    "            d[\"month_cos\"]  = np.cos(2*np.pi * d[\"month\"]/12)\n",
    "            d[\"is_weekend\"] = int(d[\"day_week\"]>=5)\n",
    "            d[\"rate_x_dist\"]= d[\"rate\"] * d[\"distance\"]\n",
    "            rows.append(d)\n",
    "        df_f = pd.DataFrame(rows)\n",
    "        return df_f.drop(columns=[\"day_week\",\"month\"])\n",
    "\n",
    "numeric_feats     = [\"rate\",\"distance\",\"day_sin\",\"day_cos\",\"month_sin\",\"month_cos\",\"is_weekend\",\"rate_x_dist\"]\n",
    "categorical_feats = [\"origin_state\",\"dest_state\",\"carrier_service\"]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numeric_feats),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_feats),\n",
    "], remainder=\"drop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 7: Train/Test split\n",
    "test_n = 500\n",
    "test_samples = [df[df[\"incidence\"]==c].sample(test_n, random_state=42) for c in [0,1]]\n",
    "test_df = pd.concat(test_samples).sample(frac=1, random_state=42)\n",
    "train_df = df.drop(test_df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a3ad8",
   "metadata": {},
   "source": [
    "## Incidencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eaf897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "▶ Mejores parámetros incidencia: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 500}\n",
      "▶ Umbral F2.0-opt: 0.1517 (F2.0 = 0.5638)\n",
      "▶ Umbral precisión ≥ 0.80: 0.9166 (prec = 0.8006, recall = 0.2605)\n",
      "▶ Máximo recall con threshold ≥ 0.25: 0.6622 (umbral = 0.2501)\n",
      "Guardados wrappers con thresholds:\n",
      "  defecto=0.3, F2.0=0.1517, prec(0.8)=0.9166, maxrec=0.2501\n",
      "\n",
      "=== Evaluación: Default ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7154    0.9300    0.8087       100\n",
      "         1.0     0.9000    0.6300    0.7412       100\n",
      "\n",
      "    accuracy                         0.7800       200\n",
      "   macro avg     0.8077    0.7800    0.7749       200\n",
      "weighted avg     0.8077    0.7800    0.7749       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93  7]\n",
      " [37 63]]\n",
      "\n",
      "=== Evaluación: F2.0-opt ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7658    0.8500    0.8057       100\n",
      "         1.0     0.8315    0.7400    0.7831       100\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.7986    0.7950    0.7944       200\n",
      "weighted avg     0.7986    0.7950    0.7944       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[85 15]\n",
      " [26 74]]\n",
      "\n",
      "=== Evaluación: Prec≥0.8 ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5833    0.9800    0.7313       100\n",
      "         1.0     0.9375    0.3000    0.4545       100\n",
      "\n",
      "    accuracy                         0.6400       200\n",
      "   macro avg     0.7604    0.6400    0.5929       200\n",
      "weighted avg     0.7604    0.6400    0.5929       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98  2]\n",
      " [70 30]]\n",
      "\n",
      "=== Evaluación: MaxRec ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7222    0.9100    0.8053       100\n",
      "         1.0     0.8784    0.6500    0.7471       100\n",
      "\n",
      "    accuracy                         0.7800       200\n",
      "   macro avg     0.8003    0.7800    0.7762       200\n",
      "weighted avg     0.8003    0.7800    0.7762       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[91  9]\n",
      " [35 65]]\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Bloque completo: GridSearch + SMOTE + Evaluación con thresholds configurables\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix\n",
    "\n",
    "# — Parámetros de configuración al inicio —\n",
    "param_grid    = {\n",
    "    \"n_estimators\":  [100, 200, 300, 500],\n",
    "    \"max_depth\":     [3, 6, 10],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "}\n",
    "cv            = 3\n",
    "default_thr   = 0.3       # umbral por defecto\n",
    "beta          = 2.0       # β para Fβ-score\n",
    "precision_min = 0.8       # precisión mínima deseada para el “Caso Precision” (5c)\n",
    "threshold_min = 0.25       # umbral mínimo para el “Caso MaxRec” (5d)\n",
    "\n",
    "# 1) Split train/validation\n",
    "train_sub_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"incidence\"]\n",
    ")\n",
    "\n",
    "# 2) Preprocesado UNA sola vez\n",
    "X_raw_train = RawToFeatures().fit_transform(train_sub_df)\n",
    "X_pre_train = preprocessor.fit_transform(X_raw_train)\n",
    "y_train     = train_sub_df[\"incidence\"]\n",
    "\n",
    "X_raw_val = RawToFeatures().transform(val_df)\n",
    "X_pre_val = preprocessor.transform(X_raw_val)\n",
    "y_val     = val_df[\"incidence\"]\n",
    "\n",
    "# 3) SMOTE una sola vez\n",
    "smote      = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_pre_train, y_train)\n",
    "\n",
    "# 4) GridSearch sobre XGBClassifier con scale_pos_weight ajustado\n",
    "n_cand = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"Fitting {cv} folds for each of {n_cand} candidates, totalling {cv * n_cand} fits\")\n",
    "\n",
    "neg, pos = len(y_res) - y_res.sum(), y_res.sum()\n",
    "clf = XGBClassifier(\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(neg/pos) * 1.5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid = GridSearchCV(clf, param_grid, scoring=\"f1\", cv=cv, verbose=1)\n",
    "grid.fit(X_res, y_res)\n",
    "\n",
    "best_clf = grid.best_estimator_\n",
    "print(\"▶ Mejores parámetros incidencia:\", grid.best_params_)\n",
    "\n",
    "# 5) Predicciones en VALIDATION\n",
    "probs_val = best_clf.predict_proba(X_pre_val)[:, 1]\n",
    "\n",
    "# — 5a) Umbral por defecto —\n",
    "y_def = (probs_val >= default_thr).astype(int)\n",
    "\n",
    "# — 5b) Umbral óptimo Fβ (β = 2.0) —\n",
    "prec, rec, thr = precision_recall_curve(y_val, probs_val)\n",
    "f2_scores      = (1 + beta**2) * (prec * rec) / (beta**2 * prec + rec + 1e-9)\n",
    "idx_f2         = np.argmax(f2_scores)\n",
    "thr_f2         = thr[idx_f2]\n",
    "print(f\"▶ Umbral F{beta}-opt: {thr_f2:.4f} (F{beta} = {f2_scores[idx_f2]:.4f})\")\n",
    "\n",
    "# — 5c) Caso Precision: umbral que garantiza al menos precision_min y maximiza recall —\n",
    "cands_prec = [(p, r, t) for p, r, t in zip(prec, rec, thr) if p >= precision_min]\n",
    "if cands_prec:\n",
    "    p_sel, r_sel, thr_prec = max(cands_prec, key=lambda x: x[1])  # max recall\n",
    "    print(f\"▶ Umbral precisión ≥ {precision_min:.2f}: {thr_prec:.4f} (prec = {p_sel:.4f}, recall = {r_sel:.4f})\")\n",
    "else:\n",
    "    # fallback al umbral que más se acerca a precision_min\n",
    "    idx_fb    = np.argmin([abs(p - precision_min) for p in prec])\n",
    "    thr_prec  = thr[idx_fb]\n",
    "    r_sel     = rec[idx_fb]\n",
    "    p_sel     = prec[idx_fb]\n",
    "    print(f\"⚠️ No hay umbral con precisión ≥ {precision_min:.2f};\")\n",
    "    print(f\"   mejor aproximación: {thr_prec:.4f} (prec = {p_sel:.4f}, recall = {r_sel:.4f})\")\n",
    "\n",
    "# — 5d) Caso MaxRec puro (threshold ≥ threshold_min) —\n",
    "cands_rec = [(r, t) for r, t in zip(rec, thr) if t >= threshold_min]\n",
    "if cands_rec:\n",
    "    r_maxrec, thr_maxrec = max(cands_rec, key=lambda x: x[0])\n",
    "    print(f\"▶ Máximo recall con threshold ≥ {threshold_min}: {r_maxrec:.4f} (umbral = {thr_maxrec:.4f})\")\n",
    "else:\n",
    "    thr_maxrec = threshold_min\n",
    "    idx_fb     = np.argmin(abs(thr - threshold_min))\n",
    "    r_maxrec   = rec[idx_fb]\n",
    "    print(f\"⚠️ No hay umbral ≥ {threshold_min}; usando {thr_maxrec:.4f} (recall = {r_maxrec:.4f})\")\n",
    "\n",
    "# 6) Definir wrappers para cada modelo (pipeline + umbral)\n",
    "class PipelineWithThreshold:\n",
    "    def __init__(self, pipeline, threshold):\n",
    "        self.pipeline  = pipeline\n",
    "        self.threshold = threshold\n",
    "    def predict(self, X):\n",
    "        probs = self.pipeline.predict_proba(X)[:,1]\n",
    "        return (probs >= self.threshold).astype(int)\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"raw\",  RawToFeatures()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\",  best_clf)\n",
    "])\n",
    "\n",
    "default_model = PipelineWithThreshold(full_pipeline, default_thr)\n",
    "f2_model      = PipelineWithThreshold(full_pipeline, thr_f2)\n",
    "prec_model    = PipelineWithThreshold(full_pipeline, thr_prec)\n",
    "maxrec_model  = PipelineWithThreshold(full_pipeline, thr_maxrec)\n",
    "\n",
    "# Guardar wrappers\n",
    "joblib.dump(default_model, \"xgb_pipeline_default_thr.pkl\")\n",
    "joblib.dump(f2_model,      \"xgb_pipeline_f2_thr.pkl\")\n",
    "joblib.dump(prec_model,    \"xgb_pipeline_prec_thr.pkl\")\n",
    "joblib.dump(maxrec_model,  \"xgb_pipeline_maxrec_thr.pkl\")\n",
    "\n",
    "print(f\"Guardados wrappers con thresholds:\\n\"\n",
    "      f\"  defecto={default_thr}, F{beta}={thr_f2:.4f}, \"\n",
    "      f\"prec({precision_min})={thr_prec:.4f}, maxrec={thr_maxrec:.4f}\")\n",
    "\n",
    "# 7) Impresión de métricas sobre test_df\n",
    "\n",
    "y_true = test_df[\"incidence\"]\n",
    "\n",
    "for name, model in [\n",
    "    (\"Default\", default_model),\n",
    "    (f\"F{beta}-opt\",  f2_model),\n",
    "    (f\"Prec≥{precision_min}\", prec_model),\n",
    "    (\"MaxRec\",        maxrec_model)\n",
    "]:\n",
    "    y_pred = model.predict(test_df)\n",
    "    print(f\"\\n=== Evaluación: {name} ===\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1e4f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mensaje de texto enviado.\n"
     ]
    }
   ],
   "source": [
    "notificar_telegram(\"Incidencia listo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f292463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluación: Default (0.3) ===\n",
      "Umbral = 0.3000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8727    0.9600    0.9143       100\n",
      "         1.0     0.9556    0.8600    0.9053       100\n",
      "\n",
      "    accuracy                         0.9100       200\n",
      "   macro avg     0.9141    0.9100    0.9098       200\n",
      "weighted avg     0.9141    0.9100    0.9098       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[96  4]\n",
      " [14 86]]\n",
      "\n",
      "=== Evaluación: F1 max (0.1732) ===\n",
      "Umbral = 0.1732\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9394    0.9300    0.9347       100\n",
      "         1.0     0.9307    0.9400    0.9353       100\n",
      "\n",
      "    accuracy                         0.9350       200\n",
      "   macro avg     0.9350    0.9350    0.9350       200\n",
      "weighted avg     0.9350    0.9350    0.9350       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93  7]\n",
      " [ 6 94]]\n",
      "\n",
      "=== Evaluación: Max Recall (0.0000) ===\n",
      "Umbral = 0.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0000    0.0000    0.0000       100\n",
      "         1.0     0.5000    1.0000    0.6667       100\n",
      "\n",
      "    accuracy                         0.5000       200\n",
      "   macro avg     0.2500    0.5000    0.3333       200\n",
      "weighted avg     0.2500    0.5000    0.3333       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 100]\n",
      " [  0 100]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim\\OneDrive\\Documents\\Programs\\2025\\Logistics\\Ventas\\Recomendaciones Buho\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Karim\\OneDrive\\Documents\\Programs\\2025\\Logistics\\Ventas\\Recomendaciones Buho\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Karim\\OneDrive\\Documents\\Programs\\2025\\Logistics\\Ventas\\Recomendaciones Buho\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Bloque 9: Evaluación Incidencia con 3 Thresholds\n",
    "inc_pipeline  = joblib.load(\"xgb_pipeline.pkl\")\n",
    "\n",
    "# 1) Predecir probabilidades de incidencia\n",
    "probs  = inc_pipeline.predict_proba(test_df)[:, 1]\n",
    "y_true = test_df[\"incidence\"]\n",
    "\n",
    "# 2) Threshold por defecto\n",
    "default_thr = 0.3\n",
    "\n",
    "# 3) Threshold óptimo para F1\n",
    "prec, rec, thr = precision_recall_curve(y_true, probs)\n",
    "f1_scores = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "best_idx  = np.argmax(f1_scores)\n",
    "f1_thr    = thr[best_idx]\n",
    "\n",
    "# 4) Threshold que maximiza recall puro\n",
    "rec_idx = np.argmax(rec)\n",
    "rec_thr = thr[rec_idx]\n",
    "\n",
    "# 5) Diccionario de thresholds\n",
    "thresholds = {\n",
    "    f\"Default ({default_thr})\"   : default_thr,\n",
    "    f\"F1 max ({f1_thr:.4f})\"     : f1_thr,\n",
    "    f\"Max Recall ({rec_thr:.4f})\": rec_thr\n",
    "}\n",
    "\n",
    "# 6) Imprimir métricas para cada caso\n",
    "for name, t in thresholds.items():\n",
    "    y_pred = (probs >= t).astype(int)\n",
    "    print(f\"\\n=== Evaluación: {name} ===\")\n",
    "    print(f\"Umbral = {t:.4f}\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6556a98",
   "metadata": {},
   "source": [
    "## Tiempo de entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0082b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "▶ Mejores parámetros Tiempo de Entrega: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.6}\n",
      "✅ Guardado xgbreg_pipeline.pkl\n",
      "\n",
      "=== Evaluación TEST Tiempo de Entrega ===\n",
      "MAE: 0.7795506119728088\n",
      "RMSE: 1.3308660607244196\n",
      "R²: 0.22786325216293335\n"
     ]
    }
   ],
   "source": [
    "# 4) Precomputar features para regresión sobre todo train_df\n",
    "X_raw_reg = RawToFeatures().fit_transform(train_df)\n",
    "X_pre_reg = preprocessor.fit_transform(X_raw_reg)\n",
    "\n",
    "# 5) Target transformado (sqrt)\n",
    "y_time = np.sqrt(train_df[\"delivery_time_bd\"])\n",
    "\n",
    "\n",
    "# — Bloque X: GridSearch+RandomizedSearch para tiempo de entrega con TODOS los datos —\n",
    "\n",
    "# 1) Ya deberías tener:\n",
    "#    X_pre_reg  (matriz de features: rate, distance, day_sin/cos, month_sin/cos, is_weekend, rate_x_dist,\n",
    "#                plus todas las dummies de origin_state, dest_state, carrier_service)\n",
    "#    y_time     (√ de delivery_time_bd)\n",
    "\n",
    "# 2) Define el regresor y el espacio de búsqueda\n",
    "reg = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "param_dist = {\n",
    "    \"n_estimators\":     [100, 200, 300, 500],\n",
    "    \"max_depth\":        [None, 5, 10, 15],\n",
    "    \"learning_rate\":    [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    \"subsample\":        [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"gamma\":            [0, 0.1, 0.5, 1],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"reg_alpha\":        [0, 0.1, 1],\n",
    "    \"reg_lambda\":       [1, 5, 10],\n",
    "}\n",
    "\n",
    "# 3) RandomizedSearchCV SOBRE X_pre_reg Y y_time\n",
    "rand = RandomizedSearchCV(\n",
    "    reg,\n",
    "    param_dist,\n",
    "    n_iter=50,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "rand.fit(X_pre_reg, y_time)   # <— aquí usas las variables que ya existían\n",
    "\n",
    "best_reg = rand.best_estimator_\n",
    "print(\"▶ Mejores parámetros Tiempo de Entrega:\", rand.best_params_)\n",
    "\n",
    "# 4) Guardar pipeline completo\n",
    "full_time_pipeline = Pipeline([\n",
    "    (\"raw\",  RawToFeatures()),\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"reg\",  best_reg)\n",
    "])\n",
    "joblib.dump(full_time_pipeline, \"xgbreg_pipeline.pkl\")\n",
    "print(\"✅ Guardado xgbreg_pipeline.pkl\")\n",
    "\n",
    "\n",
    "# 5) Evaluación rápida sobre test_df:\n",
    "X_raw_test = RawToFeatures().transform(test_df)\n",
    "X_pre_test = preprocessor.transform(X_raw_test)\n",
    "y_test     = test_df[\"delivery_time_bd\"]\n",
    "\n",
    "pred_sqrt  = best_reg.predict(X_pre_test)\n",
    "pred_days  = np.square(pred_sqrt)\n",
    "\n",
    "print(\"\\n=== Evaluación TEST Tiempo de Entrega ===\")\n",
    "print(\"MAE:\",  mean_absolute_error(y_test, pred_days))\n",
    "mse  = mean_squared_error(y_test, pred_days)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R²:\",   r2_score(y_test, pred_days))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb566caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mensaje de texto enviado.\n"
     ]
    }
   ],
   "source": [
    "notificar_telegram(\"Entrenamiento completado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
